DataParams

dataset_name = "wikitext"
debug_crop = 10
lines_per_sample = 10
max_seq_len = 100

TrainParams 

batch_size = 8
epochs = 2
no_wandb = False
val_check_epoch_frac = 0.5
lora_rank = 3

ModelParams

n_layers = 1
n_heads = 1
dim = 123
layer_architecture = "torch-transformer"
from_os_pretrained = "gpt2"